{"format": "torch", "nodes": [{"name": "shared", "id": 140544844064800, "class_name": "Embedding(32128, 512)", "parameters": [["weight", [32128, 512]]], "output_shape": [[16, 27, 512]], "num_parameters": [16449536]}, {"name": "encoder", "id": 140544844066672, "class_name": "T5Stack(\n  (embed_tokens): Embedding(32128, 512)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n            (relative_attention_bias): Embedding(32, 8)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (2): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (3): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (4): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (5): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)", "parameters": [["embed_tokens.weight", [32128, 512]], ["block.0.layer.0.SelfAttention.q.weight", [512, 512]], ["block.0.layer.0.SelfAttention.k.weight", [512, 512]], ["block.0.layer.0.SelfAttention.v.weight", [512, 512]], ["block.0.layer.0.SelfAttention.o.weight", [512, 512]], ["block.0.layer.0.SelfAttention.relative_attention_bias.weight", [32, 8]], ["block.0.layer.0.layer_norm.weight", [512]], ["block.0.layer.1.DenseReluDense.wi.weight", [2048, 512]], ["block.0.layer.1.DenseReluDense.wo.weight", [512, 2048]], ["block.0.layer.1.layer_norm.weight", [512]], ["block.1.layer.0.SelfAttention.q.weight", [512, 512]], ["block.1.layer.0.SelfAttention.k.weight", [512, 512]], ["block.1.layer.0.SelfAttention.v.weight", [512, 512]], ["block.1.layer.0.SelfAttention.o.weight", [512, 512]], ["block.1.layer.0.layer_norm.weight", [512]], ["block.1.layer.1.DenseReluDense.wi.weight", [2048, 512]], ["block.1.layer.1.DenseReluDense.wo.weight", [512, 2048]], ["block.1.layer.1.layer_norm.weight", [512]], ["block.2.layer.0.SelfAttention.q.weight", [512, 512]], ["block.2.layer.0.SelfAttention.k.weight", [512, 512]], ["block.2.layer.0.SelfAttention.v.weight", [512, 512]], ["block.2.layer.0.SelfAttention.o.weight", [512, 512]], ["block.2.layer.0.layer_norm.weight", [512]], ["block.2.layer.1.DenseReluDense.wi.weight", [2048, 512]], ["block.2.layer.1.DenseReluDense.wo.weight", [512, 2048]], ["block.2.layer.1.layer_norm.weight", [512]], ["block.3.layer.0.SelfAttention.q.weight", [512, 512]], ["block.3.layer.0.SelfAttention.k.weight", [512, 512]], ["block.3.layer.0.SelfAttention.v.weight", [512, 512]], ["block.3.layer.0.SelfAttention.o.weight", [512, 512]], ["block.3.layer.0.layer_norm.weight", [512]], ["block.3.layer.1.DenseReluDense.wi.weight", [2048, 512]], ["block.3.layer.1.DenseReluDense.wo.weight", [512, 2048]], ["block.3.layer.1.layer_norm.weight", [512]], ["block.4.layer.0.SelfAttention.q.weight", [512, 512]], ["block.4.layer.0.SelfAttention.k.weight", [512, 512]], ["block.4.layer.0.SelfAttention.v.weight", [512, 512]], ["block.4.layer.0.SelfAttention.o.weight", [512, 512]], ["block.4.layer.0.layer_norm.weight", [512]], ["block.4.layer.1.DenseReluDense.wi.weight", [2048, 512]], ["block.4.layer.1.DenseReluDense.wo.weight", [512, 2048]], ["block.4.layer.1.layer_norm.weight", [512]], ["block.5.layer.0.SelfAttention.q.weight", [512, 512]], ["block.5.layer.0.SelfAttention.k.weight", [512, 512]], ["block.5.layer.0.SelfAttention.v.weight", [512, 512]], ["block.5.layer.0.SelfAttention.o.weight", [512, 512]], ["block.5.layer.0.layer_norm.weight", [512]], ["block.5.layer.1.DenseReluDense.wi.weight", [2048, 512]], ["block.5.layer.1.DenseReluDense.wo.weight", [512, 2048]], ["block.5.layer.1.layer_norm.weight", [512]], ["final_layer_norm.weight", [512]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0]]], "num_parameters": [16449536, 262144, 262144, 262144, 262144, 256, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 512]}, {"name": "decoder", "id": 140544844066576, "class_name": "T5Stack(\n  (embed_tokens): Embedding(32128, 512)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n            (relative_attention_bias): Embedding(32, 8)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (2): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (3): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (4): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (5): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=512, bias=False)\n            (k): Linear(in_features=512, out_features=512, bias=False)\n            (v): Linear(in_features=512, out_features=512, bias=False)\n            (o): Linear(in_features=512, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=512, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)", "parameters": [["embed_tokens.weight", [32128, 512]], ["block.0.layer.0.SelfAttention.q.weight", [512, 512]], ["block.0.layer.0.SelfAttention.k.weight", [512, 512]], ["block.0.layer.0.SelfAttention.v.weight", [512, 512]], ["block.0.layer.0.SelfAttention.o.weight", [512, 512]], ["block.0.layer.0.SelfAttention.relative_attention_bias.weight", [32, 8]], ["block.0.layer.0.layer_norm.weight", [512]], ["block.0.layer.1.EncDecAttention.q.weight", [512, 512]], ["block.0.layer.1.EncDecAttention.k.weight", [512, 512]], ["block.0.layer.1.EncDecAttention.v.weight", [512, 512]], ["block.0.layer.1.EncDecAttention.o.weight", [512, 512]], ["block.0.layer.1.layer_norm.weight", [512]], ["block.0.layer.2.DenseReluDense.wi.weight", [2048, 512]], ["block.0.layer.2.DenseReluDense.wo.weight", [512, 2048]], ["block.0.layer.2.layer_norm.weight", [512]], ["block.1.layer.0.SelfAttention.q.weight", [512, 512]], ["block.1.layer.0.SelfAttention.k.weight", [512, 512]], ["block.1.layer.0.SelfAttention.v.weight", [512, 512]], ["block.1.layer.0.SelfAttention.o.weight", [512, 512]], ["block.1.layer.0.layer_norm.weight", [512]], ["block.1.layer.1.EncDecAttention.q.weight", [512, 512]], ["block.1.layer.1.EncDecAttention.k.weight", [512, 512]], ["block.1.layer.1.EncDecAttention.v.weight", [512, 512]], ["block.1.layer.1.EncDecAttention.o.weight", [512, 512]], ["block.1.layer.1.layer_norm.weight", [512]], ["block.1.layer.2.DenseReluDense.wi.weight", [2048, 512]], ["block.1.layer.2.DenseReluDense.wo.weight", [512, 2048]], ["block.1.layer.2.layer_norm.weight", [512]], ["block.2.layer.0.SelfAttention.q.weight", [512, 512]], ["block.2.layer.0.SelfAttention.k.weight", [512, 512]], ["block.2.layer.0.SelfAttention.v.weight", [512, 512]], ["block.2.layer.0.SelfAttention.o.weight", [512, 512]], ["block.2.layer.0.layer_norm.weight", [512]], ["block.2.layer.1.EncDecAttention.q.weight", [512, 512]], ["block.2.layer.1.EncDecAttention.k.weight", [512, 512]], ["block.2.layer.1.EncDecAttention.v.weight", [512, 512]], ["block.2.layer.1.EncDecAttention.o.weight", [512, 512]], ["block.2.layer.1.layer_norm.weight", [512]], ["block.2.layer.2.DenseReluDense.wi.weight", [2048, 512]], ["block.2.layer.2.DenseReluDense.wo.weight", [512, 2048]], ["block.2.layer.2.layer_norm.weight", [512]], ["block.3.layer.0.SelfAttention.q.weight", [512, 512]], ["block.3.layer.0.SelfAttention.k.weight", [512, 512]], ["block.3.layer.0.SelfAttention.v.weight", [512, 512]], ["block.3.layer.0.SelfAttention.o.weight", [512, 512]], ["block.3.layer.0.layer_norm.weight", [512]], ["block.3.layer.1.EncDecAttention.q.weight", [512, 512]], ["block.3.layer.1.EncDecAttention.k.weight", [512, 512]], ["block.3.layer.1.EncDecAttention.v.weight", [512, 512]], ["block.3.layer.1.EncDecAttention.o.weight", [512, 512]], ["block.3.layer.1.layer_norm.weight", [512]], ["block.3.layer.2.DenseReluDense.wi.weight", [2048, 512]], ["block.3.layer.2.DenseReluDense.wo.weight", [512, 2048]], ["block.3.layer.2.layer_norm.weight", [512]], ["block.4.layer.0.SelfAttention.q.weight", [512, 512]], ["block.4.layer.0.SelfAttention.k.weight", [512, 512]], ["block.4.layer.0.SelfAttention.v.weight", [512, 512]], ["block.4.layer.0.SelfAttention.o.weight", [512, 512]], ["block.4.layer.0.layer_norm.weight", [512]], ["block.4.layer.1.EncDecAttention.q.weight", [512, 512]], ["block.4.layer.1.EncDecAttention.k.weight", [512, 512]], ["block.4.layer.1.EncDecAttention.v.weight", [512, 512]], ["block.4.layer.1.EncDecAttention.o.weight", [512, 512]], ["block.4.layer.1.layer_norm.weight", [512]], ["block.4.layer.2.DenseReluDense.wi.weight", [2048, 512]], ["block.4.layer.2.DenseReluDense.wo.weight", [512, 2048]], ["block.4.layer.2.layer_norm.weight", [512]], ["block.5.layer.0.SelfAttention.q.weight", [512, 512]], ["block.5.layer.0.SelfAttention.k.weight", [512, 512]], ["block.5.layer.0.SelfAttention.v.weight", [512, 512]], ["block.5.layer.0.SelfAttention.o.weight", [512, 512]], ["block.5.layer.0.layer_norm.weight", [512]], ["block.5.layer.1.EncDecAttention.q.weight", [512, 512]], ["block.5.layer.1.EncDecAttention.k.weight", [512, 512]], ["block.5.layer.1.EncDecAttention.v.weight", [512, 512]], ["block.5.layer.1.EncDecAttention.o.weight", [512, 512]], ["block.5.layer.1.layer_norm.weight", [512]], ["block.5.layer.2.DenseReluDense.wi.weight", [2048, 512]], ["block.5.layer.2.DenseReluDense.wo.weight", [512, 2048]], ["block.5.layer.2.layer_norm.weight", [512]], ["final_layer_norm.weight", [512]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0], [[0], 0, 0, 0, 0, [0], 0, [0], 0, [0], 0, 0, [0], 0, 0]]], "num_parameters": [16449536, 262144, 262144, 262144, 262144, 256, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 262144, 262144, 262144, 262144, 512, 262144, 262144, 262144, 262144, 512, 1048576, 1048576, 512, 512]}, {"name": "lm_head", "id": 140544843728640, "class_name": "Linear(in_features=512, out_features=32128, bias=False)", "parameters": [["weight", [32128, 512]]], "output_shape": [[16, 109, 32128]], "num_parameters": [16449536]}], "edges": []}